{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">MIS 382N: Advanced Predictive Modeling</p>\n",
    "# <p style=\"text-align: center;\">Assignment 5</p>\n",
    "## <p style=\"text-align: center;\">Total points: 50 </p>\n",
    "## <p style=\"text-align: center;\">Due: Mon, November 28</p>\n",
    "\n",
    "\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. Please submit **only one** ipynb file from each group, and include the names of all the group members. Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Members: Zack Bilderback, Estevan Gonzales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Random Forest vs Boosting - Regression (15pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we will compare performance of different ensemble methods for regression problems: [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html), [Gradient Boosting Regressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) (GBR), and [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor). Board game data set from DataQuest will be used (you can download data from Canvas: 'games.csv').\n",
    "\n",
    "1. (1) Load the data, (2) remove duplicate rows, (3) remove features of type string (object in pandas), and (4) replace missing values by mean of each column. Then, partition data into features (X) and the target label (y) for regression task. We want to predict the *average_rating*. Use [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split data into training and testing: test_size=0.33, random_state=42. (1pt)\n",
    "\n",
    "2. Use a [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) to predict average_rating. Find the best parameters (including *n_estimators*) using [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Report the accuracy of your model in terms of RMSE. (4pts)\n",
    "\n",
    "3. Use [Gradient Boosting Regressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) (GBR), and [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor) for predicting the targets. Again, find the best parameters (including *n_estimators,* and* learning_rate*), and report corresponding RMSE for each algorithm. (8pts)\n",
    "\n",
    "4. Which model did you expect to be more accurate in predicting the targets? Why? Did your observation match this expectation? (2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import (train_test_split,GridSearchCV)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import (RandomForestRegressor,GradientBoostingRegressor,AdaBoostRegressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the data and remove duplicates\n",
    "data = pd.read_csv('games.csv').drop_duplicates()\n",
    "\n",
    "# Gets index of all numeric data and make new dataframe with that data\n",
    "numeric_feats = data.dtypes[data.dtypes != \"object\"].index\n",
    "allNumeric = data[numeric_feats]\n",
    "\n",
    "# filling NA's with the mean of the column:\n",
    "allNumeric = allNumeric.fillna(allNumeric.mean())\n",
    "\n",
    "label_name = 'average_rating'\n",
    "y = allNumeric[label_name]\n",
    "X = allNumeric.drop(label_name,axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 10, 25, 50, 100, 200]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'n_estimators':[5, 10, 25, 50, 100, 200]}\n",
    "rfr = RandomForestRegressor()\n",
    "clf = GridSearchCV(rfr, parameters)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Best parameters: {'n_estimators': 200}\n",
      "RandomForest Best score: 0.892490162827\n"
     ]
    }
   ],
   "source": [
    "print \"{}: {}\".format(\"RandomForest Best parameters\", clf.best_params_)\n",
    "print \"{}: {}\".format(\"RandomForest Best score\", clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Train RMSE: 0.368868781574\n",
      "RandomForest Test RMSE: 0.993109765279\n"
     ]
    }
   ],
   "source": [
    "# In sample RMSE\n",
    "Ytrain_pred = clf.predict(X_train)\n",
    "Ytrain_mse = mean_squared_error(y_train, Ytrain_pred)\n",
    "print \"{}: {}\".format('RandomForest Train RMSE', np.sqrt(Ytrain_mse))\n",
    "\n",
    "# Out of sample RMSE\n",
    "Ytest_pred = clf.predict(X_test)\n",
    "Ytest_mse = mean_squared_error(y_test, Ytest_pred)\n",
    "print \"{}: {}\".format('RandomForest Test RMSE', np.sqrt(Ytest_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "             warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 10, 25, 50, 100, 200], 'learning_rate': [1.0, 0.5, 0.1, 0.01, 0.001, 0.0001]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'n_estimators':[5, 10, 25, 50, 100, 200], 'learning_rate':[1.0, 0.5, 0.1, 0.01, 0.001, 0.0001]}\n",
    "\n",
    "adr = AdaBoostRegressor()\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "adr_clf = GridSearchCV(adr, parameters)\n",
    "gbr_clf = GridSearchCV(gbr, parameters)\n",
    "\n",
    "adr_clf.fit(X_train, y_train)\n",
    "gbr_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Best parameters: {'n_estimators': 50, 'learning_rate': 0.1}\n",
      "AdaBoost Best score: 0.862395519354\n"
     ]
    }
   ],
   "source": [
    "print \"{}: {}\".format(\"AdaBoost Best parameters\", adr_clf.best_params_)\n",
    "print \"{}: {}\".format(\"AdaBoost Best score\", adr_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoost Best parameters: {'n_estimators': 200, 'learning_rate': 0.5}\n",
      "GradientBoost Best score: 0.891381377369\n"
     ]
    }
   ],
   "source": [
    "print \"{}: {}\".format(\"GradientBoost Best parameters\", gbr_clf.best_params_)\n",
    "print \"{}: {}\".format(\"GradientBoost Best score\", gbr_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Train RMSE: 1.12991352282\n",
      "AdaBoost Test RMSE: 1.13359305794\n"
     ]
    }
   ],
   "source": [
    "# In sample RMSE\n",
    "Ytrain_pred = adr_clf.predict(X_train)\n",
    "Ytrain_mse = mean_squared_error(y_train, Ytrain_pred)\n",
    "print \"{}: {}\".format('AdaBoost Train RMSE', np.sqrt(Ytrain_mse))\n",
    "\n",
    "# Out of sample RMSE\n",
    "Ytest_pred = adr_clf.predict(X_test)\n",
    "Ytest_mse = mean_squared_error(y_test, Ytest_pred)\n",
    "print \"{}: {}\".format('AdaBoost Test RMSE', np.sqrt(Ytest_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoost Train RMSE: 0.95195870728\n",
      "GradientBoost Test RMSE: 1.00651340389\n"
     ]
    }
   ],
   "source": [
    "# In sample RMSE\n",
    "Ytrain_pred = gbr_clf.predict(X_train)\n",
    "Ytrain_mse = mean_squared_error(y_train, Ytrain_pred)\n",
    "print \"{}: {}\".format('GradientBoost Train RMSE', np.sqrt(Ytrain_mse))\n",
    "\n",
    "# Out of sample RMSE\n",
    "Ytest_pred = gbr_clf.predict(X_test)\n",
    "Ytest_mse = mean_squared_error(y_test, Ytest_pred)\n",
    "print \"{}: {}\".format('GradientBoost Test RMSE', np.sqrt(Ytest_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thought the GradientBoostingRegressor model would be the most accurate because it is known to be very powerful and almost always performs the best out of all models so this was our initial guess. If we look at the scores of each model, the GradientBoostingRegressor and RandomForest performed the best but the RandomForest model had the best test RMSE. I think it is overfitting however since there is a big difference between the in-sample RMSE and out-of-sample RMSE. With more fine tuning, the GradientBoostingRegressor would most likely perform the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Random Forest vs Boosting - Classification (15 pts)\n",
    "In this question, we will compare performance of different ensemble methods for classification problems: [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), [Gradient Boosting Decision Tree](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) (GBDT), and [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier). [Spam Classification Data](https://archive.ics.uci.edu/ml/datasets/Spambase) of UCI will be used (you can download data from Canvas: 'spam_uci.csv'). Don't worry about column names. The last column represents target label, 1 if spam and zero otherwise.\n",
    "\n",
    "1. Load the data and partition it into features (X) and the target label (y) for classification task. Then, use [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split data into training and testing: test_size=0.33, random_state=42. (1pt)\n",
    "\n",
    "2. Use a [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to classify whether an email is spam. Find the best parameters (including *n_estimators* and *criterion*) using [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Report your testing accuracy ([accuracy_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)) and [roc_auc_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score). You will need [predict_proba](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict_proba) for roc_auc_score. (4pts)\n",
    "\n",
    "3. Use [Gradient Boosting Decision Tree](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) (GBDT), and [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) for the spam classification problem. Again, find the best parameters (including *n_estimators, learning_rate,* and *max_depth (GBDT only)*), and report corresponding accuracy_score and roc_auc_score on the test data for each algorithm. (8pts)\n",
    "\n",
    "4. Point out one advantage and one disadvantage of Random Forest compared to GBDT (2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import (train_test_split,GridSearchCV)\n",
    "from sklearn.metrics import (accuracy_score,roc_auc_score,roc_curve)\n",
    "from sklearn.ensemble import (RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the data and remove duplicates\n",
    "data = pd.read_csv('spam_uci.csv')\n",
    "\n",
    "# last column name is '57'\n",
    "label_name = '57'\n",
    "y = data[label_name]\n",
    "X = data.drop(label_name,axis=1)\n",
    "\n",
    "# split into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state=42)\n",
    "#X_train = X_train.values\n",
    "#X_test = X_test.values\n",
    "#y_train = y_train.values\n",
    "\n",
    "# reshape for gridsearch cv's expected data shape\n",
    "#c = y_train.shape\n",
    "#y_train = y_train.reshape(c,)\n",
    "#y_test = y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 10, 15, 25, 50], 'criterion': ['gini', 'entropy']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'n_estimators':[5, 10, 15, 25, 50], 'criterion':['gini', 'entropy']}\n",
    "rfc = RandomForestClassifier()\n",
    "rfc_clf = GridSearchCV(rfc, parameters)\n",
    "rfc_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Best parameters: {'n_estimators': 50, 'criterion': 'gini'}\n",
      "RandomForest Best score: 1.0\n"
     ]
    }
   ],
   "source": [
    "print \"{}: {}\".format(\"RandomForest Best parameters\", rfc_clf.best_params_)\n",
    "print \"{}: {}\".format(\"RandomForest Best score\", rfc_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Train RMSE: 0.0\n",
      "RandomForest Test RMSE: 0.0\n"
     ]
    }
   ],
   "source": [
    "# In sample RMSE\n",
    "Ytrain_pred = rfc_clf.predict(X_train)\n",
    "Ytrain_mse = mean_squared_error(y_train, Ytrain_pred)\n",
    "print \"{}: {}\".format('RandomForest Train RMSE', np.sqrt(Ytrain_mse))\n",
    "\n",
    "# Out of sample RMSE\n",
    "Ytest_pred = rfc_clf.predict(X_test)\n",
    "Ytest_mse = mean_squared_error(y_test, Ytest_pred)\n",
    "print \"{}: {}\".format('RandomForest Test RMSE', np.sqrt(Ytest_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators = 50, criterion = 'gini')\n",
    "model = rfc.fit(X_train, y_train)\n",
    "Ytest_pred = model.predict(X_test)\n",
    "pred_probs = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RocCurve(predprobs, actual_y):\n",
    "    '''predicted probabilities should be in form of an array\n",
    "    \n",
    "    function returns the fpr, tpr, threshold and the AUC of the ROC Curve as the score'''\n",
    "    \n",
    "    pos_probs = []\n",
    "    for i in predprobs:\n",
    "        pos_probs.append(i[1])\n",
    "    fpr, tpr, threshold = roc_curve(actual_y, pos_probs)\n",
    "    score = roc_auc_score(actual_y, pos_probs)\n",
    "    return fpr, tpr, threshold, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC_score: 1.0\n",
      "accuracy score: 0.999341672153\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, threshold, score = RocCurve(pred_probs, y_test)\n",
    "print \"{}: {}\".format('AUC_score', score)\n",
    "ACC = accuracy_score(y_test, Ytest_pred) \n",
    "print \"{}: {}\".format('accuracy score', ACC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=100, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [2, 5, 10, 25, 50, 100, 200], 'learning_rate': [1.0, 0.5, 0.1, 0.01, 0.001, 0.0001], 'max_depth': [1, 2, 3, 5, 7, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adc_parameters = {'n_estimators':[2, 5, 10, 25, 50, 100, 200], 'learning_rate':[1.0, 0.5, 0.1, 0.01, 0.001, 0.0001]}\n",
    "gbc_parameters = {'n_estimators':[2, 5, 10, 25, 50, 100, 200], 'learning_rate':[1.0, 0.5, 0.1, 0.01, 0.001, 0.0001], \n",
    "                  'max_depth':[1, 2, 3, 5, 7, 10]}\n",
    "\n",
    "adc = AdaBoostClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "adc_clf = GridSearchCV(adc, adc_parameters)\n",
    "gbc_clf = GridSearchCV(gbc, gbc_parameters)\n",
    "\n",
    "adc_clf.fit(X_train, y_train)\n",
    "gbc_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Best parameters: {'n_estimators': 2, 'learning_rate': 1.0}\n",
      "AdaBoost Best score: 1.0\n"
     ]
    }
   ],
   "source": [
    "print \"{}: {}\".format(\"AdaBoost Best parameters\", adc_clf.best_params_)\n",
    "print \"{}: {}\".format(\"AdaBoost Best score\", adc_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoost Best parameters: {'n_estimators': 2, 'learning_rate': 1.0, 'max_depth': 1}\n",
      "GradientBoost Best score: 1.0\n"
     ]
    }
   ],
   "source": [
    "print \"{}: {}\".format(\"GradientBoost Best parameters\", gbc_clf.best_params_)\n",
    "print \"{}: {}\".format(\"GradientBoost Best score\", gbc_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Train RMSE: 0.0\n",
      "AdaBoost Test RMSE: 0.0362857505715\n"
     ]
    }
   ],
   "source": [
    "# In sample RMSE\n",
    "Ytrain_pred = adc_clf.predict(X_train)\n",
    "Ytrain_mse = mean_squared_error(y_train, Ytrain_pred)\n",
    "print \"{}: {}\".format('AdaBoost Train RMSE', np.sqrt(Ytrain_mse))\n",
    "\n",
    "# Out of sample RMSE\n",
    "Ytest_pred = adc_clf.predict(X_test)\n",
    "Ytest_mse = mean_squared_error(y_test, Ytest_pred)\n",
    "print \"{}: {}\".format('AdaBoost Test RMSE', np.sqrt(Ytest_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoost Train RMSE: 0.0\n",
      "GradientBoost Test RMSE: 0.0362857505715\n"
     ]
    }
   ],
   "source": [
    "# In sample RMSE\n",
    "Ytrain_pred = gbc_clf.predict(X_train)\n",
    "Ytrain_mse = mean_squared_error(y_train, Ytrain_pred)\n",
    "print \"{}: {}\".format('GradientBoost Train RMSE', np.sqrt(Ytrain_mse))\n",
    "\n",
    "# Out of sample RMSE\n",
    "Ytest_pred = gbc_clf.predict(X_test)\n",
    "Ytest_mse = mean_squared_error(y_test, Ytest_pred)\n",
    "print \"{}: {}\".format('GradientBoost Test RMSE', np.sqrt(Ytest_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adc = AdaBoostClassifier(n_estimators = 2, learning_rate = 1.0)\n",
    "gbc = GradientBoostingClassifier(n_estimators = 2, learning_rate = 1.0, max_depth = 1)\n",
    "\n",
    "adc_model = adc.fit(X_train, y_train)\n",
    "gbc_model = gbc.fit(X_train, y_train)\n",
    "\n",
    "adc_Ytest_pred = adc_model.predict(X_test)\n",
    "gbc_Ytest_pred = gbc_model.predict(X_test)\n",
    "\n",
    "adc_pred_probs = adc_model.predict_proba(X_test)\n",
    "gbc_pred_probs = gbc_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost AUC_score: 0.998871331828\n",
      "AdaBoost accuracy score: 0.998683344305\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, threshold, score = RocCurve(adc_pred_probs, y_test)\n",
    "print \"{}: {}\".format('AdaBoost AUC_score', score)\n",
    "ACC = accuracy_score(y_test, adc_Ytest_pred) \n",
    "print \"{}: {}\".format('AdaBoost accuracy score', ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoost AUC_score: 0.998871331828\n",
      "GradientBoost accuracy score: 0.998683344305\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, threshold, score = RocCurve(gbc_pred_probs, y_test)\n",
    "print \"{}: {}\".format('GradientBoost AUC_score', score)\n",
    "ACC = accuracy_score(y_test, gbc_Ytest_pred) \n",
    "print \"{}: {}\".format('GradientBoost accuracy score', ACC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One advantage of Random Forests over GBDT is that the training can be highly parallelized.  Each tree (and subtree) can be trained on independent hardware. This allows you to scale it to massive data that you would have to subsample for other techniques, and you can use it for online training where additional data is constantly streaming in. One disadvantage of Random Forests compared to GBDT is that GBDT typically outperforms Random Forests at the expense of complexity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Question 3 - Matrix Factorization for Rating Prediction (20pts)\n",
    "\n",
    "The movielens dataset contains 1 million movie ratings from several thousand users. We will be using *k*-rank matrix factorization to estimate this dataset as the product $X=UV^T$, where *U* and *V* have only $k$ columns.\n",
    "\n",
    "1) You can download the movielens 1M dataset from https://datahub.io/dataset/movielens, but for this problem use the data available on Canvas. It has been split into training and test sets, and converted to matrix format where the rows correspond to users and the columns to movies. Note that most of the entries are NaNs, indicating that these ratings are missing. An extra file, lens1m_361M_titles.csv, has been added so you can check out specific movies if you're curious.\n",
    "\n",
    "2) Scikit-learn is a little behind for recommender systems, and doesn't have any method to factorize matrices with missing data. Which means you get to code it! Slide 22 of the 'apa large scale learning' lecture notes has the equations for stochastic gradient descent on *U* and *V*. You will have to:\n",
    "* Set up initial guesses for the *U* and *V* matrices. I suggest small random values.\n",
    "* Find a suitable learning rate for the descent. A learning rate that is too large will probably blow up, like in HW3 problem 1.\n",
    "* Come up with a stopping policy\n",
    "* Code the descent algorithm (5 pts)\n",
    "\n",
    "3) Using your SGD algorithm, apply 2-rank matrix factorization on the filled training matrix. Calculate the RMSE of this model on the training data and on the test data (separately). The optimal score on the training data is around .86 RMSE; your version of gradient descent must go at least below .91 RMSE. (5 pts)\n",
    "\n",
    "4) You should notice some overfitting. Because matrix factorization learns separate scores for each movie, a movie with very few reviews may be easily overfit. You may want to only predict ratings when you have enough information to reach a good conclusion. Recalculate the RMSE on the test data, specifically for movies with at least 50 reviews (don't retrain the models). Also report the percent of movies that are still included (after cutting those with < 50 reviews), and the percent of test ratings that are still included. (5 pts)\n",
    "\n",
    "5) Repeat steps 3 and 4 with 5-rank factorization. Display training and test RMSE. (5 pts)\n",
    "\n",
    "Hints:  \n",
    "The numpy function *nanmean* is helpful for RMSE calculation.  \n",
    "The descent algorithm will probably run for at least several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "titles = pd.read_csv('lens1m_361M_titles.csv')\n",
    "test_X = np.load('lens1m_361M_test.npy')\n",
    "train_X = np.load('lens1m_361M_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
