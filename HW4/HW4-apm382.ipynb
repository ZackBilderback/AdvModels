{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">MIS 382N: Advanced Predictive Modeling</p>\n",
    "# <p style=\"text-align: center;\">Assignment 4</p>\n",
    "## <p style=\"text-align: center;\">Total points: 50 </p>\n",
    "## <p style=\"text-align: center;\">Due: Mon, November 14</p>\n",
    "\n",
    "\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. Please submit **only one** ipynb file from each group, and include the names of all the group members. Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - MNIST (15 pts)\n",
    "In this question you classify hand-written digits. We want to use MNIST data set and try Multi-layer Perceptron (MLP) classifier using sklearn package in Python. In order to simplify the problem, we classify digits into 8 classes (digits 0,1,2,...7) and ignore records for digits 8 and 9.  Use the code below to access the data set and extract data with labels 0 to 7, and split the data set into train set and test set.\n",
    "\n",
    "1. Fit a Multilayer Perceptron Classifier using the standard options on sklearn's MLP on train data. Report the root MSE for both train and test data. (5 pts)\n",
    "\n",
    "    Use these parameters for your model: \n",
    "                    {hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=42,\n",
    "                    learning_rate_init=.1}\n",
    "                    \n",
    "2. To find better parameters for the MLP Classifier model, try an exhaustive search over all parameters of the data. Use sklearn's GridSearchCV to find the best subset of parameters from the set:\n",
    "                    { alpha = [0.1,0.01,0.001], activation : ['logistic', 'relu'] }\n",
    "    which parameters resulted in a more accurate model? Can you explain why? (5 pts)\n",
    "\n",
    "3. Select 5 misclassified images and display them. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import (train_test_split,KFold)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mnist = fetch_mldata(\"MNIST original\")\n",
    "# rescale the data, use the traditional train/test split\n",
    "X = (mnist.data / 255.)[:48200]\n",
    "y = mnist.target[:48200]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#first we scale our x variables\n",
    "X_scaler = StandardScaler()\n",
    "X_train = X_scaler.fit_transform(X_train)\n",
    "X_test = X_scaler.transform(X_test)\n",
    "MLP = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4, solver='sgd', \n",
    "                    verbose=10, tol=1e-4, random_state=42, learning_rate_init=.1)\n",
    "mse = make_scorer(mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.22278030\n",
      "Iteration 2, loss = 0.13945856\n",
      "Iteration 3, loss = 0.09964869\n",
      "Iteration 4, loss = 0.08568452\n",
      "Iteration 5, loss = 0.06914200\n",
      "Iteration 6, loss = 0.05917721\n",
      "Iteration 7, loss = 0.05847170\n",
      "Iteration 8, loss = 0.04573220\n",
      "Iteration 9, loss = 0.04211366\n",
      "Iteration 10, loss = 0.03337099\n",
      "RMSE on training data: 0.3057765662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zack_\\Anaconda2\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.22278030\n",
      "Iteration 2, loss = 0.13945856\n",
      "Iteration 3, loss = 0.09964869\n",
      "Iteration 4, loss = 0.08568452\n",
      "Iteration 5, loss = 0.06914200\n",
      "Iteration 6, loss = 0.05917721\n",
      "Iteration 7, loss = 0.05847170\n",
      "Iteration 8, loss = 0.04573220\n",
      "Iteration 9, loss = 0.04211366\n",
      "Iteration 10, loss = 0.03337099\n",
      "RMSE on test data: 0.543349073587\n"
     ]
    }
   ],
   "source": [
    "#getting training RMSe\n",
    "trainmodel = MLP.fit(X_train, y_train)\n",
    "Ytrain_pred = trainmodel.predict(X_train)\n",
    "train_score = mean_squared_error(y_train, Ytrain_pred)\n",
    "print 'RMSE on training data:', np.sqrt(train_score)\n",
    "\n",
    "#getting testing RMSE\n",
    "testmodel = MLP.fit(X_train, y_train)\n",
    "Ytest_pred = testmodel.predict(X_test)\n",
    "test_score = mean_squared_error(y_test, Ytest_pred)\n",
    "print 'RMSE on test data:', np.sqrt(test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zack_\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\zack_\\Anaconda2\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import grid_search\n",
    "parameters = { 'alpha' : [0.1,0.01,0.001], 'activation' : ['logistic', 'relu'] }\n",
    "clf = grid_search.GridSearchCV(MLP, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.39414805\n",
      "Iteration 2, loss = 0.18690024\n",
      "Iteration 3, loss = 0.16444825\n",
      "Iteration 4, loss = 0.15234852\n",
      "Iteration 5, loss = 0.14367744\n",
      "Iteration 6, loss = 0.13744477\n",
      "Iteration 7, loss = 0.13286077\n",
      "Iteration 8, loss = 0.12939549\n",
      "Iteration 9, loss = 0.12595149\n",
      "Iteration 10, loss = 0.12362577\n",
      "Iteration 1, loss = 0.39716252\n",
      "Iteration 2, loss = 0.18894011\n",
      "Iteration 3, loss = 0.16705155\n",
      "Iteration 4, loss = 0.15364257\n",
      "Iteration 5, loss = 0.14493847\n",
      "Iteration 6, loss = 0.13865362\n",
      "Iteration 7, loss = 0.13413443\n",
      "Iteration 8, loss = 0.13021769\n",
      "Iteration 9, loss = 0.12754604\n",
      "Iteration 10, loss = 0.12524274\n",
      "Iteration 1, loss = 0.38957297\n",
      "Iteration 2, loss = 0.18457611\n",
      "Iteration 3, loss = 0.16283999\n",
      "Iteration 4, loss = 0.14969508\n",
      "Iteration 5, loss = 0.14018918\n",
      "Iteration 6, loss = 0.13421350\n",
      "Iteration 7, loss = 0.12970408\n",
      "Iteration 8, loss = 0.12619187\n",
      "Iteration 9, loss = 0.12321463\n",
      "Iteration 10, loss = 0.12076311\n",
      "Iteration 1, loss = 0.36928324\n",
      "Iteration 2, loss = 0.15093654\n",
      "Iteration 3, loss = 0.12215728\n",
      "Iteration 4, loss = 0.10479046\n",
      "Iteration 5, loss = 0.09146685\n",
      "Iteration 6, loss = 0.08121661\n",
      "Iteration 7, loss = 0.07317124\n",
      "Iteration 8, loss = 0.06658686\n",
      "Iteration 9, loss = 0.06053411\n",
      "Iteration 10, loss = 0.05569305\n",
      "Iteration 1, loss = 0.37227302\n",
      "Iteration 2, loss = 0.15286116\n",
      "Iteration 3, loss = 0.12427390\n",
      "Iteration 4, loss = 0.10561861\n",
      "Iteration 5, loss = 0.09237862\n",
      "Iteration 6, loss = 0.08220428\n",
      "Iteration 7, loss = 0.07412075\n",
      "Iteration 8, loss = 0.06711591\n",
      "Iteration 9, loss = 0.06163189\n",
      "Iteration 10, loss = 0.05672428\n",
      "Iteration 1, loss = 0.36467098\n",
      "Iteration 2, loss = 0.14853102\n",
      "Iteration 3, loss = 0.12017992\n",
      "Iteration 4, loss = 0.10168800\n",
      "Iteration 5, loss = 0.08758529\n",
      "Iteration 6, loss = 0.07767186\n",
      "Iteration 7, loss = 0.06959156\n",
      "Iteration 8, loss = 0.06310699\n",
      "Iteration 9, loss = 0.05750286\n",
      "Iteration 10, loss = 0.05288961\n",
      "Iteration 1, loss = 0.36669708\n",
      "Iteration 2, loss = 0.14697986\n",
      "Iteration 3, loss = 0.11728432\n",
      "Iteration 4, loss = 0.09908093\n",
      "Iteration 5, loss = 0.08496446\n",
      "Iteration 6, loss = 0.07398269\n",
      "Iteration 7, loss = 0.06523565\n",
      "Iteration 8, loss = 0.05797593\n",
      "Iteration 9, loss = 0.05130967\n",
      "Iteration 10, loss = 0.04588351\n",
      "Iteration 1, loss = 0.36968461\n",
      "Iteration 2, loss = 0.14889302\n",
      "Iteration 3, loss = 0.11934603\n",
      "Iteration 4, loss = 0.09986035\n",
      "Iteration 5, loss = 0.08582933\n",
      "Iteration 6, loss = 0.07496274\n",
      "Iteration 7, loss = 0.06617167\n",
      "Iteration 8, loss = 0.05850672\n",
      "Iteration 9, loss = 0.05241545\n",
      "Iteration 10, loss = 0.04690101\n",
      "Iteration 1, loss = 0.36208122\n",
      "Iteration 2, loss = 0.14456842\n",
      "Iteration 3, loss = 0.11526762\n",
      "Iteration 4, loss = 0.09590996\n",
      "Iteration 5, loss = 0.08101211\n",
      "Iteration 6, loss = 0.07036581\n",
      "Iteration 7, loss = 0.06157963\n",
      "Iteration 8, loss = 0.05439637\n",
      "Iteration 9, loss = 0.04818647\n",
      "Iteration 10, loss = 0.04302112\n",
      "Iteration 1, loss = 0.28937851\n",
      "Iteration 2, loss = 0.16122994\n",
      "Iteration 3, loss = 0.12899186\n",
      "Iteration 4, loss = 0.11802834\n",
      "Iteration 5, loss = 0.10531929\n",
      "Iteration 6, loss = 0.08742922\n",
      "Iteration 7, loss = 0.08039324\n",
      "Iteration 8, loss = 0.07111500\n",
      "Iteration 9, loss = 0.06812695\n",
      "Iteration 10, loss = 0.05682520\n",
      "Iteration 1, loss = 0.28784540\n",
      "Iteration 2, loss = 0.17452566\n",
      "Iteration 3, loss = 0.13993963\n",
      "Iteration 4, loss = 0.14688657\n",
      "Iteration 5, loss = 0.12974068\n",
      "Iteration 6, loss = 0.11306360\n",
      "Iteration 7, loss = 0.09588991\n",
      "Iteration 8, loss = 0.08056033\n",
      "Iteration 9, loss = 0.08083392\n",
      "Iteration 10, loss = 0.07254912\n",
      "Iteration 1, loss = 0.27588037\n",
      "Iteration 2, loss = 0.15711159\n",
      "Iteration 3, loss = 0.13563817\n",
      "Iteration 4, loss = 0.11247910\n",
      "Iteration 5, loss = 0.10290646\n",
      "Iteration 6, loss = 0.08867445\n",
      "Iteration 7, loss = 0.07362650\n",
      "Iteration 8, loss = 0.06693608\n",
      "Iteration 9, loss = 0.05907167\n",
      "Iteration 10, loss = 0.05507908\n",
      "Iteration 1, loss = 0.25607467\n",
      "Iteration 2, loss = 0.11846139\n",
      "Iteration 3, loss = 0.08317685\n",
      "Iteration 4, loss = 0.08032873\n",
      "Iteration 5, loss = 0.05931567\n",
      "Iteration 6, loss = 0.04986583\n",
      "Iteration 7, loss = 0.03357366\n",
      "Iteration 8, loss = 0.02627609\n",
      "Iteration 9, loss = 0.02264814\n",
      "Iteration 10, loss = 0.02052974\n",
      "Iteration 1, loss = 0.25293102\n",
      "Iteration 2, loss = 0.13384083\n",
      "Iteration 3, loss = 0.12684722\n",
      "Iteration 4, loss = 0.13434953\n",
      "Iteration 5, loss = 0.09405154\n",
      "Iteration 6, loss = 0.08191392\n",
      "Iteration 7, loss = 0.07015363\n",
      "Iteration 8, loss = 0.06762224\n",
      "Iteration 9, loss = 0.04721970\n",
      "Iteration 10, loss = 0.04229074\n",
      "Iteration 1, loss = 0.24384243\n",
      "Iteration 2, loss = 0.12214765\n",
      "Iteration 3, loss = 0.10050734\n",
      "Iteration 4, loss = 0.07109401\n",
      "Iteration 5, loss = 0.06723191\n",
      "Iteration 6, loss = 0.05868684\n",
      "Iteration 7, loss = 0.04667697\n",
      "Iteration 8, loss = 0.03707430\n",
      "Iteration 9, loss = 0.03238533\n",
      "Iteration 10, loss = 0.03192828\n",
      "Iteration 1, loss = 0.25255421\n",
      "Iteration 2, loss = 0.11819568\n",
      "Iteration 3, loss = 0.08763917\n",
      "Iteration 4, loss = 0.06431600\n",
      "Iteration 5, loss = 0.04933044\n",
      "Iteration 6, loss = 0.03812718\n",
      "Iteration 7, loss = 0.03235264\n",
      "Iteration 8, loss = 0.02369324\n",
      "Iteration 9, loss = 0.01363432\n",
      "Iteration 10, loss = 0.00729483\n",
      "Iteration 1, loss = 0.24926889\n",
      "Iteration 2, loss = 0.12828826\n",
      "Iteration 3, loss = 0.13121932\n",
      "Iteration 4, loss = 0.16532293\n",
      "Iteration 5, loss = 0.12465673\n",
      "Iteration 6, loss = 0.08331649\n",
      "Iteration 7, loss = 0.06930817\n",
      "Iteration 8, loss = 0.07546735\n",
      "Iteration 9, loss = 0.07003121\n",
      "Iteration 10, loss = 0.04846719\n",
      "Iteration 1, loss = 0.24031323\n",
      "Iteration 2, loss = 0.11748553\n",
      "Iteration 3, loss = 0.09808286\n",
      "Iteration 4, loss = 0.06633545\n",
      "Iteration 5, loss = 0.05049250\n",
      "Iteration 6, loss = 0.03812954\n",
      "Iteration 7, loss = 0.02800514\n",
      "Iteration 8, loss = 0.03029467\n",
      "Iteration 9, loss = 0.02578814\n",
      "Iteration 10, loss = 0.02458253\n",
      "Iteration 1, loss = 0.26428646\n",
      "Iteration 2, loss = 0.18628626\n",
      "Iteration 3, loss = 0.15212917\n",
      "Iteration 4, loss = 0.13354154\n",
      "Iteration 5, loss = 0.12389902\n",
      "Iteration 6, loss = 0.10821480\n",
      "Iteration 7, loss = 0.10142063\n",
      "Iteration 8, loss = 0.09010754\n",
      "Iteration 9, loss = 0.08155460\n",
      "Iteration 10, loss = 0.07933508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(50,), learning_rate='constant',\n",
       "       learning_rate_init=0.1, max_iter=10, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=42, shuffle=True,\n",
       "       solver='sgd', tol=0.0001, validation_fraction=0.1, verbose=10,\n",
       "       warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': [0.1, 0.01, 0.001], 'activation': ['logistic', 'relu']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:{'alpha': 0.1, 'activation': 'relu'}\n",
      "Best score:0.974412171508\n"
     ]
    }
   ],
   "source": [
    "print \"{}:{}\".format(\"Best parameters\", clf.best_params_)\n",
    "print \"{}{}\".format(\"Best score:\",clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The best parameters are (activation='relu', alpha=0.1). They actually performed worse than the initial settings which seems a bit confusing but the 'relu' activation function performs better because it reduces the likelihood of the gradient to vanish and it represents hidden units sparsely. Logistic regression on the other hand tends to make the gradient vanish as the absolute values of x increases. Also, sigmoids are always likely to generate some non-zero value resulting in dense representations for the hidden units. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.22278030\n",
      "Iteration 2, loss = 0.13945856\n",
      "Iteration 3, loss = 0.09964869\n",
      "Iteration 4, loss = 0.08568452\n",
      "Iteration 5, loss = 0.06914200\n",
      "Iteration 6, loss = 0.05917721\n",
      "Iteration 7, loss = 0.05847170\n",
      "Iteration 8, loss = 0.04573220\n",
      "Iteration 9, loss = 0.04211366\n",
      "Iteration 10, loss = 0.03337099\n",
      "Sample Missclassifications:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAC7CAYAAAAgy4bYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsmYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13606668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MLP = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4, solver='sgd', \n",
    "                    verbose=10, tol=1e-4, random_state=42, learning_rate_init=.1)\n",
    "\n",
    "testmodel = MLP.fit(X_train, y_train)\n",
    "Ytest_pred = testmodel.predict(X_test)\n",
    "\n",
    "misclass_idx = np.where(y_test != Ytest_pred)\n",
    "\n",
    "misclass_act = y_test[misclass_idx]\n",
    "misclass_pred = Ytest_pred[misclass_idx]\n",
    "misclass_img = X_test[misclass_idx]*255\n",
    "misclass_img = misclass_img.astype(int)\n",
    "\n",
    "misclass_list = zip(misclass_act,misclass_pred,misclass_img)\n",
    "\n",
    "print \"Sample Missclassifications:\"\n",
    "for index, missclass in enumerate(misclass_list[:5]):\n",
    "    plt.subplot(3, 3, index+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(missclass[2].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Act: %i | Pred: %i' % (missclass[0], missclass[1]))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Question 2: Regression Trees (10 points)\n",
    "\n",
    "In this question, we will be exploring the application of regression tree (RT) using sklearn package in Python. You will be using the same Hitters.csv dataset (available on Canvas) used in HW2 Q5 to predict a baseball player’s Salary using all the 16 performance variables. Use a random state of 42 and a test size of 1/3 to split the data into training and test.\n",
    "\n",
    "1. Build a regression using [DecisionTreeRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) with max_depth = 5. Report the mean squared errors on both training and test datasets. (4)\n",
    "2. Repeat Part-1 with max_depth = 2. (4)\n",
    "3. Briefly explain what you observe from these MSE values obtained by using maximum tree depths 5 and 2? Which tree is better and why? (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hitters = pd.read_csv(\"hitters.csv\")\n",
    "target = hitters['Salary']\n",
    "del hitters['Salary']\n",
    "X_train, X_test, y_train, y_test = train_test_split(hitters, target, test_size=0.333, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DTR = DecisionTreeRegressor(max_depth=5)\n",
    "model = DTR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data MSE: 18749.4789728\n",
      "Test data MSE: 211972.033541\n"
     ]
    }
   ],
   "source": [
    "#training data\n",
    "Ytrain_pred = model.predict(X_train)\n",
    "train_MSE = mean_squared_error(y_train, Ytrain_pred)\n",
    "print \"Training data MSE:\", train_MSE\n",
    "\n",
    "#test data\n",
    "Ypred_test = model.predict(X_test)\n",
    "test_MSE = mean_squared_error(y_test, Ypred_test)\n",
    "print \"Test data MSE:\", test_MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DTR = DecisionTreeRegressor(max_depth=2)\n",
    "model = DTR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data MSE: 69909.5877084\n",
      "Test data MSE: 166165.705731\n"
     ]
    }
   ],
   "source": [
    "#training data\n",
    "Ytrain_pred = model.predict(X_train)\n",
    "train_MSE = mean_squared_error(y_train, Ytrain_pred)\n",
    "print \"Training data MSE:\", train_MSE\n",
    "\n",
    "#test data\n",
    "Ypred_test = model.predict(X_test)\n",
    "test_MSE = mean_squared_error(y_test, Ypred_test)\n",
    "print \"Test data MSE:\", test_MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree with max depth of 5 performs better in-sample which makes us think that it is overfitting the data because it then performs poorly on the test data. The decision tree with max depth of 2 is better for the test data because it keeps the splits simple and does not over fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Question 3 - Support Vector Regression vs. Linear Regression (10pts)\n",
    "Download datasets 'summer_gpa_test.csv' and 'summer_gpa_train.csv' from Canvas. With this toy dataset, we want to predict GPA in Summer 2016 for students using 5 different features. So, target variable will be 'GPA_summer2016' in this problem.\n",
    "\n",
    "1. Fit a support vector regression using the default options on [sklearn's SVR](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) on training data. Note that the default kernel is \"rbf\".  Report the root MSE for both train and test data. (2)\n",
    "\n",
    "2. Fit SVR with 'linear' kernel and Linear Regression (for other options, use default parameter settings). Report RMSE of prediction on train and test data for the two methods. (3)\n",
    "\n",
    "3. Now, compare the results of three different methods, then provide a possible reason for SVR with RBF kernel not working well on test set. (1)\n",
    "\n",
    "4.  Provide simple residual plots on Train and Test set for all three methods. Specifically, submit a scatter plot wherein y-axis shows the residuals and x-axis the predicted values. What can you learn about the effect of outliers on different models from this problem? (4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "d_train = pd.read_csv('summer_gpa_train.csv',index_col='student')\n",
    "d_test = pd.read_csv('summer_gpa_test.csv',index_col='student')\n",
    "\n",
    "y_train = d_train['GPA_summer2016']\n",
    "y_test = d_test['GPA_summer2016']\n",
    "x_train = d_train.drop('GPA_summer2016',axis=1)\n",
    "x_test = d_test.drop('GPA_summer2016',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Bayesian Networks (10 pts)\n",
    "#### Part (a) (5 points)\n",
    "Consider the following Bayesian network of binary (True/False) variables.\n",
    "\n",
    "<img src=\"hw4q4pic1.png\">\n",
    "\n",
    "This is equivalent to saying that X and Y are conditionally independent given C, or P(X,Y|C) = P(X|C)P(Y|C). This happens to be the assumption used by the Naive Bayes classifier.\n",
    "\n",
    "The exact probabilities are given:  \n",
    "P(X = True | C = True) = .75  \n",
    "P(X = True | C = False) = .5  \n",
    "P(Y = True | C = True) = .25  \n",
    "P(Y = True | C = False) = .5  \n",
    "P(C = True) = .5\n",
    "\n",
    "Find P(C = True | X = True, Y = True). The easiest method is to use Bayes rule, along with the conditional independence equation given above.\n",
    "\n",
    "#### (b) (5 points)\n",
    "A new feature Z is added, and based on prior knowledge, we believe that one of the two networks given below properly captures the dependencies among the variables. Our goal is to determine P(C|X,Y,Z). For each of these two different networks:\n",
    "\n",
    "<img src=\"hw4q4pic2.png\">\n",
    "\n",
    "will P(C|X,Y,Z) be the same as P(C|X,Y) (the inference from part a)? Or will it be different? Give a separate answer for each network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Mulitclass Classification (5 points)\n",
    "\n",
    "One way of using a binary classifier for addressing a multiclass classification problem is to use a One-vs.-All (or One-vs.-Rest)  strategy.\n",
    "\n",
    "1. Briefly describe the  One-vs.-All method.  (2pts)\n",
    "\n",
    "2. What are two disadvantages of using the One-vs.-All method in situations where the number of classes $N$ is very large? (3pts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
